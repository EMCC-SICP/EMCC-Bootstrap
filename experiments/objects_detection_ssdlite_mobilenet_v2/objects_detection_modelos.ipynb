{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objects Detection (SSDLite, MobileNetV2, COCO)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - ü§ñ See [full list of Machine Learning Experiments](https://github.com/trekhleb/machine-learning-experiments) on **GitHub**<br/><br/>\n",
    "> - ‚ñ∂Ô∏è **Interactive Demo**: [try this model and other machine learning experiments in action](https://trekhleb.github.io/machine-learning-experiments/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment we will use pre-trained [ssdlite_mobilenet_v2_coco](http://download.tensorflow.org/models/object_detection/ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz) model from [Tensorflow detection models zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) to do objects detection on the photos.\n",
    "\n",
    "![objects_detection_ssdlite_mobilenet_v2.jpg](../../demos/src/images/objects_detection_ssdlite_mobilenet_v2.jpg)\n",
    "\n",
    "_This notebook is inspired by [Objects Detection API Demo](https://colab.research.google.com/github/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb)_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Dependencies\n",
    "\n",
    "- [tensorflow](https://www.tensorflow.org/) - for developing and training ML models.\n",
    "- [matplotlib](https://matplotlib.org/) - for plotting the data.\n",
    "- [numpy](https://numpy.org/) - for linear algebra operations.\n",
    "- [cv2](https://pypi.org/project/opencv-python/) - for processing the images and drawing object detections on top of them.\n",
    "- [PIL](https://pypi.org/project/Pillow/2.2.1/) - for convenient image loading.\n",
    "- [pathlib](https://docs.python.org/3/library/pathlib.html) - for working with model files.\n",
    "- [math](https://docs.python.org/3/library/math.html) - to do simple math operations while drawing the detection frames.\n",
    "- [google.protobuf](https://pypi.org/project/protobuf/) - for reading the files in protobuf format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorflow_version` not found.\n"
     ]
    }
   ],
   "source": [
    "# Selecting Tensorflow version v2 (the command is relevant for Colab only).\n",
    "%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.10.10\n",
      "Tensorflow version: 2.11.0\n",
      "Keras version: 2.11.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "  \n",
    "# appending a path\n",
    "sys.path.append(\"C:/Users/Gmun/AppData/Local/Programs/Python/Python310/modelos/SICP/Lib/site-packages/tensorflow\")  \n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import cv2 \n",
    "import math\n",
    "from PIL import Image\n",
    "from google.protobuf import text_format\n",
    "import platform\n",
    "\n",
    "print('Python version:', platform.python_version())\n",
    "print('Tensorflow version:', tf.__version__)\n",
    "print('Keras version:', tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model\n",
    "\n",
    "To do objects detection we're going to use [ssdlite_mobilenet_v2_coco](http://download.tensorflow.org/models/object_detection/ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz) model from [Tensorflow detection models zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md).\n",
    "\n",
    "The full name of the model will be **ssdlite_mobilenet_v2_coco_2018_05_09**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: no se puede crear el directorio ¬´.tmp¬ª: El archivo ya existe\n"
     ]
    }
   ],
   "source": [
    "# Create cache folder.\n",
    "!mkdir .tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the module from internet, unpacks it and initializes a Tensorflow saved model.\n",
    "def load_model(model_name):\n",
    "    model_url = 'http://download.tensorflow.org/models/object_detection/' + model_name + '.tar.gz'\n",
    "    \n",
    "    model_dir = tf.keras.utils.get_file(\n",
    "        fname=model_name, \n",
    "        origin=model_url,\n",
    "        untar=True,\n",
    "        cache_dir=pathlib.Path('.tmp').absolute()\n",
    "    )\n",
    "    model = tf.saved_model.load(model_dir + '/saved_model')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL_NAME = 'ssdlite_mobilenet_v2_coco_2018_05_09'\n",
    "\n",
    "#saved_model = load_model(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_downloaded(model_dir):\n",
    "    model = tf.saved_model.load(model_dir)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "                                                                                    #tiempo que se demora en cargar el modelo\n",
    "centernet = './Modelos/centernet_resnet101_v1_fpn_512x512_coco17_tpu-8/saved_model' #17.6s\n",
    "efficiendet_d1 = './Modelos/efficientdet_d1_coco17_tpu-32/saved_model'              #24.6s\n",
    "ssd = './Modelos/ssd_mobilenet_v2_320x320_coco17_tpu-8/saved_model'                 #9.9s\n",
    "\n",
    "ssdlite = './Modelos/ssdlite_mobilenet_v2_coco_2018_05_09/saved_model'              #4.9s\n",
    "\n",
    "\n",
    "MODEL_DOWNLOADED = ssdlite\n",
    "\n",
    "saved_model = load_model_downloaded(MODEL_DOWNLOADED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_SignatureMap({'serving_default': <ConcreteFunction pruned(inputs) at 0x7FD6D505FC70>})"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exploring model signatures.\n",
    "saved_model.signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading default model signature.\n",
    "model = saved_model.signatures['serving_default']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading model labels\n",
    "\n",
    "Depending on what dataset has been used to train the model we need to download proper labels set from [tensorflow models](https://github.com/tensorflow/models/tree/master/research/object_detection/data) repository.\n",
    "\n",
    "The **ssdlite_mobilenet_v2_coco** model has been trained on [COCO](http://cocodataset.org) dataset which has **90** objects categories. This list of categories we're going to download and explore. We need a label file with the name [mscoco_label_map.pbtxt](https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_label_map.pbtxt)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the protobuf label map\n",
    "\n",
    "Label object structure is defined in [string_int_label_map.proto](https://github.com/tensorflow/models/tree/master/research/object_detection/protos) file in [protobuf](https://developers.google.com/protocol-buffers) format.\n",
    "\n",
    "In order to convert `mscoco_label_map.pbtxt` file to Python dictionary we need to load `string_int_label_map.proto` file and compile it using `protoc`. Before doing the we need to [install](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md#manual-protobuf-compiler-installation-and-usage) `protoc`.\n",
    "\n",
    "One of the ways to **install** `protoc` is to load it manually:\n",
    "\n",
    "```\n",
    "PROTOC_ZIP=protoc-3.7.1-osx-x86_64.zip\n",
    "curl -OL https://github.com/protocolbuffers/protobuf/releases/download/v3.7.1/$PROTOC_ZIP\n",
    "sudo unzip -o $PROTOC_ZIP -d .tmp/protoc\n",
    "rm -f $PROTOC_ZIP\n",
    "```\n",
    "\n",
    "After that we may **compile** `proto` files by running:\n",
    "\n",
    "```\n",
    ".tmp/protoc/bin/protoc ./protos/*.proto --python_out=.\n",
    "```\n",
    "\n",
    "‚òùüèª For simplicity reasons we have `string_int_label_map.proto` and its compiled version `string_int_label_map_pb2.py` in the `protos` directory. So let's just include this compiled package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from protos import string_int_label_map_pb2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and parsing the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(labels_name):\n",
    "    labels_url = 'https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/data/' + labels_name\n",
    "    \n",
    "    labels_path = tf.keras.utils.get_file(\n",
    "        fname=labels_name, \n",
    "        origin=labels_url,\n",
    "        cache_dir=pathlib.Path('.tmp').absolute()\n",
    "    )\n",
    "    \n",
    "    labels_file = open(labels_path, 'r')\n",
    "    labels_string = labels_file.read()\n",
    "    \n",
    "    labels_map = string_int_label_map_pb2.StringIntLabelMap()\n",
    "    try:\n",
    "        text_format.Merge(labels_string, labels_map)\n",
    "    except text_format.ParseError:\n",
    "        labels_map.ParseFromString(labels_string)\n",
    "    \n",
    "    labels_dict = {}\n",
    "    for item in labels_map.item:\n",
    "        labels_dict[item.id] = item.display_name\n",
    "    \n",
    "    return labels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels_downloaded(labels_name):\n",
    "    \n",
    "        labels_file = open(labels_name, 'r')\n",
    "        labels_string = labels_file.read()\n",
    "        \n",
    "        labels_map = string_int_label_map_pb2.StringIntLabelMap()\n",
    "        try:\n",
    "            text_format.Merge(labels_string, labels_map)\n",
    "        except text_format.ParseError:\n",
    "            labels_map.ParseFromString(labels_string)\n",
    "        \n",
    "        labels_dict = {}\n",
    "        for item in labels_map.item:\n",
    "            labels_dict[item.id] = item.display_name\n",
    "        \n",
    "        return labels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'person',\n",
       " 2: 'bicycle',\n",
       " 3: 'car',\n",
       " 4: 'motorcycle',\n",
       " 5: 'airplane',\n",
       " 6: 'bus',\n",
       " 7: 'train',\n",
       " 8: 'truck',\n",
       " 9: 'boat',\n",
       " 10: 'traffic light',\n",
       " 11: 'fire hydrant',\n",
       " 13: 'stop sign',\n",
       " 14: 'parking meter',\n",
       " 15: 'bench',\n",
       " 16: 'bird',\n",
       " 17: 'cat',\n",
       " 18: 'dog',\n",
       " 19: 'horse',\n",
       " 20: 'sheep',\n",
       " 21: 'cow',\n",
       " 22: 'elephant',\n",
       " 23: 'bear',\n",
       " 24: 'zebra',\n",
       " 25: 'giraffe',\n",
       " 27: 'backpack',\n",
       " 28: 'umbrella',\n",
       " 31: 'handbag',\n",
       " 32: 'tie',\n",
       " 33: 'suitcase',\n",
       " 34: 'frisbee',\n",
       " 35: 'skis',\n",
       " 36: 'snowboard',\n",
       " 37: 'sports ball',\n",
       " 38: 'kite',\n",
       " 39: 'baseball bat',\n",
       " 40: 'baseball glove',\n",
       " 41: 'skateboard',\n",
       " 42: 'surfboard',\n",
       " 43: 'tennis racket',\n",
       " 44: 'bottle',\n",
       " 46: 'wine glass',\n",
       " 47: 'cup',\n",
       " 48: 'fork',\n",
       " 49: 'knife',\n",
       " 50: 'spoon',\n",
       " 51: 'bowl',\n",
       " 52: 'banana',\n",
       " 53: 'apple',\n",
       " 54: 'sandwich',\n",
       " 55: 'orange',\n",
       " 56: 'broccoli',\n",
       " 57: 'carrot',\n",
       " 58: 'hot dog',\n",
       " 59: 'pizza',\n",
       " 60: 'donut',\n",
       " 61: 'cake',\n",
       " 62: 'chair',\n",
       " 63: 'couch',\n",
       " 64: 'potted plant',\n",
       " 65: 'bed',\n",
       " 67: 'dining table',\n",
       " 70: 'toilet',\n",
       " 72: 'tv',\n",
       " 73: 'laptop',\n",
       " 74: 'mouse',\n",
       " 75: 'remote',\n",
       " 76: 'keyboard',\n",
       " 77: 'cell phone',\n",
       " 78: 'microwave',\n",
       " 79: 'oven',\n",
       " 80: 'toaster',\n",
       " 81: 'sink',\n",
       " 82: 'refrigerator',\n",
       " 84: 'book',\n",
       " 85: 'clock',\n",
       " 86: 'vase',\n",
       " 87: 'scissors',\n",
       " 88: 'teddy bear',\n",
       " 89: 'hair drier',\n",
       " 90: 'toothbrush'}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABELS_NAME = './Label_maps/mscoco_label_map.pbtxt'\n",
    "labels = load_labels_downloaded(LABELS_NAME)\n",
    "labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 40852\n",
      "drwxrwxr-x 3 juan8ahp-ta juan8ahp-ta     4096 abr 13 16:31 .\n",
      "drwxrwxr-x 3 juan8ahp-ta juan8ahp-ta     4096 may 23 13:46 ..\n",
      "-rw-rw-r-- 1 juan8ahp-ta juan8ahp-ta       77 abr 13 16:31 checkpoint\n",
      "-rw-rw-r-- 1 juan8ahp-ta juan8ahp-ta 19911343 abr 13 16:31 frozen_inference_graph.pb\n",
      "-rw-rw-r-- 1 juan8ahp-ta juan8ahp-ta 18205188 abr 13 16:31 model.ckpt.data-00000-of-00001\n",
      "-rw-rw-r-- 1 juan8ahp-ta juan8ahp-ta    17703 abr 13 16:31 model.ckpt.index\n",
      "-rw-rw-r-- 1 juan8ahp-ta juan8ahp-ta  3665866 abr 13 16:31 model.ckpt.meta\n",
      "-rw-rw-r-- 1 juan8ahp-ta juan8ahp-ta     4199 may 29 20:45 pipeline.config\n",
      "drwxrwxr-x 2 juan8ahp-ta juan8ahp-ta     4096 abr 13 16:31 saved_model\n"
     ]
    }
   ],
   "source": [
    "# List model files\n",
    "!ls -la .tmp/datasets/ssdlite_mobilenet_v2_coco_2018_05_09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model {\n",
      "  ssd {\n",
      "    num_classes: 90\n",
      "    image_resizer {\n",
      "      fixed_shape_resizer {\n",
      "        height: 300\n",
      "        width: 300\n",
      "      }\n",
      "    }\n",
      "    feature_extractor {\n",
      "      type: \"ssd_mobilenet_v2\"\n",
      "      depth_multiplier: 1.0\n",
      "      min_depth: 16\n",
      "      conv_hyperparams {\n",
      "        regularizer {\n",
      "          l2_regularizer {\n",
      "            weight: 3.99999989895e-05\n",
      "          }\n",
      "        }\n",
      "        initializer {\n",
      "          truncated_normal_initializer {\n",
      "            mean: 0.0\n",
      "            stddev: 0.0299999993294\n",
      "          }\n",
      "        }\n",
      "        activation: RELU_6\n",
      "        batch_norm {\n",
      "          decay: 0.999700009823\n",
      "          center: true\n",
      "          scale: true\n",
      "          epsilon: 0.0010000000475\n",
      "          train: true\n",
      "        }\n",
      "      }\n",
      "      use_depthwise: true\n",
      "    }\n",
      "    box_coder {\n",
      "      faster_rcnn_box_coder {\n",
      "        y_scale: 10.0\n",
      "        x_scale: 10.0\n",
      "        height_scale: 5.0\n",
      "        width_scale: 5.0\n",
      "      }\n",
      "    }\n",
      "    matcher {\n",
      "      argmax_matcher {\n",
      "        matched_threshold: 0.5\n",
      "        unmatched_threshold: 0.5\n",
      "        ignore_thresholds: false\n",
      "        negatives_lower_than_unmatched: true\n",
      "        force_match_for_each_row: true\n",
      "      }\n",
      "    }\n",
      "    similarity_calculator {\n",
      "      iou_similarity {\n",
      "      }\n",
      "    }\n",
      "    box_predictor {\n",
      "      convolutional_box_predictor {\n",
      "        conv_hyperparams {\n",
      "          regularizer {\n",
      "            l2_regularizer {\n",
      "              weight: 3.99999989895e-05\n",
      "            }\n",
      "          }\n",
      "          initializer {\n",
      "            truncated_normal_initializer {\n",
      "              mean: 0.0\n",
      "              stddev: 0.0299999993294\n",
      "            }\n",
      "          }\n",
      "          activation: RELU_6\n",
      "          batch_norm {\n",
      "            decay: 0.999700009823\n",
      "            center: true\n",
      "            scale: true\n",
      "            epsilon: 0.0010000000475\n",
      "            train: true\n",
      "          }\n",
      "        }\n",
      "        min_depth: 0\n",
      "        max_depth: 0\n",
      "        num_layers_before_predictor: 0\n",
      "        use_dropout: false\n",
      "        dropout_keep_probability: 0.800000011921\n",
      "        kernel_size: 3\n",
      "        box_code_size: 4\n",
      "        apply_sigmoid_to_scores: false\n",
      "        use_depthwise: true\n",
      "      }\n",
      "    }\n",
      "    anchor_generator {\n",
      "      ssd_anchor_generator {\n",
      "        num_layers: 6\n",
      "        min_scale: 0.20000000298\n",
      "        max_scale: 0.949999988079\n",
      "        aspect_ratios: 1.0\n",
      "        aspect_ratios: 2.0\n",
      "        aspect_ratios: 0.5\n",
      "        aspect_ratios: 3.0\n",
      "        aspect_ratios: 0.333299994469\n",
      "      }\n",
      "    }\n",
      "    post_processing {\n",
      "      batch_non_max_suppression {\n",
      "        score_threshold: 0.300000011921\n",
      "        iou_threshold: 0.600000023842\n",
      "        max_detections_per_class: 100\n",
      "        max_total_detections: 100\n",
      "      }\n",
      "      score_converter: SIGMOID\n",
      "    }\n",
      "    normalize_loss_by_num_matches: true\n",
      "    loss {\n",
      "      localization_loss {\n",
      "        weighted_smooth_l1 {\n",
      "        }\n",
      "      }\n",
      "      classification_loss {\n",
      "        weighted_sigmoid {\n",
      "        }\n",
      "      }\n",
      "      hard_example_miner {\n",
      "        num_hard_examples: 3000\n",
      "        iou_threshold: 0.990000009537\n",
      "        loss_type: CLASSIFICATION\n",
      "        max_negatives_per_positive: 3\n",
      "        min_negatives_per_image: 3\n",
      "      }\n",
      "      classification_weight: 1.0\n",
      "      localization_weight: 1.0\n",
      "    }\n",
      "  }\n",
      "}\n",
      "train_config {\n",
      "  batch_size: 24\n",
      "  data_augmentation_options {\n",
      "    random_horizontal_flip {\n",
      "    }\n",
      "  }\n",
      "  data_augmentation_options {\n",
      "    ssd_random_crop {\n",
      "    }\n",
      "  }\n",
      "  optimizer {\n",
      "    rms_prop_optimizer {\n",
      "      learning_rate {\n",
      "        exponential_decay_learning_rate {\n",
      "          initial_learning_rate: 0.00400000018999\n",
      "          decay_steps: 800720\n",
      "          decay_factor: 0.949999988079\n",
      "        }\n",
      "      }\n",
      "      momentum_optimizer_value: 0.899999976158\n",
      "      decay: 0.899999976158\n",
      "      epsilon: 1.0\n",
      "    }\n",
      "  }\n",
      "  fine_tune_checkpoint: \"PATH_TO_BE_CONFIGURED/model.ckpt\"\n",
      "  num_steps: 200000\n",
      "  fine_tune_checkpoint_type: \"detection\"\n",
      "}\n",
      "train_input_reader {\n",
      "  label_map_path: \"PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt\"\n",
      "  tf_record_input_reader {\n",
      "    input_path: \"PATH_TO_BE_CONFIGURED/mscoco_train.record\"\n",
      "  }\n",
      "}\n",
      "eval_config {\n",
      "  num_examples: 8000\n",
      "  max_evals: 10\n",
      "  use_moving_averages: false\n",
      "}\n",
      "eval_input_reader {\n",
      "  label_map_path: \"PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt\"\n",
      "  shuffle: false\n",
      "  num_readers: 1\n",
      "  tf_record_input_reader {\n",
      "    input_path: \"PATH_TO_BE_CONFIGURED/mscoco_val.record\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Check model pipeline.\n",
    "!cat ./Modelos/ssdlite_mobilenet_v2_coco_2018_05_09/pipeline.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'image_tensor:0' shape=(None, None, None, 3) dtype=uint8>]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'detection_boxes:0' shape=(None, 100, 4) dtype=float32>,\n",
       " <tf.Tensor 'detection_classes:0' shape=(None, 100) dtype=float32>,\n",
       " <tf.Tensor 'detection_scores:0' shape=(None, 100) dtype=float32>,\n",
       " <tf.Tensor 'num_detections:0' shape=(None,) dtype=float32>]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(image_np):\n",
    "    plt.figure()\n",
    "    plt.imshow(image_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('data_prueba_j/WhatsApp Image 2022-07-27 at 1.00.06 PM (2).jpeg'),\n",
       " PosixPath('data_prueba_j/WhatsApp Image 2022-07-27 at 1.00.06 PM.jpeg'),\n",
       " PosixPath('data_prueba_j/WhatsApp Image 2022-07-27 at 1.00.07 PM (1).jpeg'),\n",
       " PosixPath('data_prueba_j/WhatsApp Image 2022-07-27 at 12.57.36 PM.jpeg'),\n",
       " PosixPath('data_prueba_j/WhatsApp Image 2022-07-27 at 12.57.37 PM (2).jpeg'),\n",
       " PosixPath('data_prueba_j/WhatsApp Image 2022-07-27 at 12.57.39 PM (1).jpeg'),\n",
       " PosixPath('data_prueba_j/WhatsApp Image 2022-07-27 at 12.57.39 PM (2).jpeg'),\n",
       " PosixPath('data_prueba_j/WhatsApp Image 2022-07-27 at 12.57.40 PM.jpeg'),\n",
       " PosixPath('data_prueba_j/WhatsApp Image 2022-07-28 at 9.25.59 PM.jpeg'),\n",
       " PosixPath('data_prueba_j/WhatsApp Image 2022-07-28 at 9.27.28 PM.jpeg'),\n",
       " PosixPath('data_prueba_j/WhatsApp Image 2022-07-28 at 9.28.42 PM.jpeg'),\n",
       " PosixPath('data_prueba_j/WhatsApp Image 2022-07-28 at 9.29.31 PM (1).jpeg'),\n",
       " PosixPath('data_prueba_j/WhatsApp Image 2022-07-28 at 9.29.31 PM (2).jpeg'),\n",
       " PosixPath('data_prueba_j/WhatsApp Image 2022-07-28 at 9.29.31 PM.jpeg'),\n",
       " PosixPath('data_prueba_j/WhatsApp Image 2022-07-28 at 9.32.24 PM.jpeg')]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_IMAGES_DIR_PATH = pathlib.Path('data_prueba_j')\n",
    "TEST_IMAGE_PATHS = sorted(list(TEST_IMAGES_DIR_PATH.glob('*.jpeg')))#cambiar si es necesario\n",
    "TEST_IMAGE_PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor image_path in TEST_IMAGE_PATHS:\\n    image_np = mpimg.imread(image_path)\\n    display_image(image_np)\\n'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for image_path in TEST_IMAGE_PATHS:\n",
    "    image_np = mpimg.imread(image_path)\n",
    "    display_image(image_np)\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para eliminar los objetos que no sean personas (Editado)\n",
    "def detect_people(detected_people):\n",
    "    indices = np.where(detected_people['detection_classes'] != 1)\n",
    "    \n",
    "    detection_scores_temp = detected_people['detection_scores']\n",
    "    detection_classes_temp = detected_people['detection_classes']\n",
    "    detection_boxes_temp = detected_people['detection_boxes']\n",
    "    num_detections_temp = detected_people['num_detections']\n",
    "\n",
    "    for i in indices[0]:\n",
    "        detection_scores_temp = np.delete(detected_people['detection_scores'], indices[0])\n",
    "        detection_classes_temp = np.delete(detected_people['detection_classes'], indices[0])\n",
    "        detection_boxes_temp = np.delete(detected_people['detection_boxes'], indices[0], axis = 0) # Elimina las filas el √≠ndice\n",
    "        num_detections_temp = detected_people['num_detections'] - len(indices[0])\n",
    "\n",
    "    detected_people['detection_scores'] = detection_scores_temp\n",
    "    detected_people['detection_classes'] = detection_classes_temp\n",
    "    detected_people['detection_boxes'] = detection_boxes_temp\n",
    "    detected_people['num_detections'] = num_detections_temp\n",
    "\n",
    "    return detected_people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_objects_on_image(image, model):\n",
    "\n",
    "    \n",
    "    image = np.asarray(image)\n",
    "    input_tensor = tf.convert_to_tensor(image)\n",
    "    # Adding one more dimension since model expect a batch of images.\n",
    "    input_tensor = input_tensor[tf.newaxis, ...]\n",
    "\n",
    "    output_dict = model(input_tensor)\n",
    "\n",
    "    num_detections = int(output_dict['num_detections'])\n",
    "    output_dict = {\n",
    "        key:value[0, :num_detections].numpy() \n",
    "        for key,value in output_dict.items()\n",
    "        if key != 'num_detections'\n",
    "    }\n",
    "    output_dict['num_detections'] = num_detections\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "    \n",
    "    return detect_people(output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_detections_on_image(image, detections, labels):\n",
    "    image_with_detections = image\n",
    "    width, height, channels = image_with_detections.shape\n",
    "    \n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    color = (0, 255, 0)\n",
    "    label_padding = 5\n",
    "\n",
    "    tamanoTexto = 1.4 # tama√±o de la etiqueta que describe la clase del objeto\n",
    "    \n",
    "    num_detections = detections['num_detections']\n",
    "    if num_detections > 0:\n",
    "        for detection_index in range(num_detections):\n",
    "            detection_score = detections['detection_scores'][detection_index]\n",
    "            detection_box = detections['detection_boxes'][detection_index]\n",
    "            detection_class = detections['detection_classes'][detection_index]\n",
    "            detection_label = labels[detection_class]\n",
    "            detection_label_full = detection_label + ' ' + str(math.floor(100 * detection_score)) + '%'\n",
    "            \n",
    "            y1 = int(width * detection_box[0])\n",
    "            x1 = int(height * detection_box[1])\n",
    "            y2 = int(width * detection_box[2])\n",
    "            x2 = int(height * detection_box[3])\n",
    "                        \n",
    "            # Detection rectangle.    \n",
    "            image_with_detections = cv2.rectangle(\n",
    "                image_with_detections,\n",
    "                (x1, y1),\n",
    "                (x2, y2),\n",
    "                color,\n",
    "                3\n",
    "            )\n",
    "            \n",
    "            # Label background.\n",
    "            label_size = cv2.getTextSize(\n",
    "                detection_label_full,\n",
    "                cv2.FONT_HERSHEY_COMPLEX,\n",
    "                tamanoTexto,\n",
    "                2\n",
    "            )\n",
    "            image_with_detections = cv2.rectangle(\n",
    "                image_with_detections,\n",
    "                (x1, y1 - label_size[0][1] - 2 * label_padding),\n",
    "                (x1 + label_size[0][0] + 2 * label_padding, y1),\n",
    "                color,\n",
    "                -1\n",
    "            )\n",
    "            \n",
    "            # Label text.\n",
    "            cv2.putText(\n",
    "                image_with_detections,\n",
    "                detection_label_full,\n",
    "                (x1 + label_padding, y1 - label_padding),\n",
    "                font,\n",
    "                tamanoTexto,\n",
    "                (0, 0, 0),\n",
    "                1,\n",
    "                cv2.LINE_AA\n",
    "            )\n",
    "            \n",
    "    return image_with_detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'detection_classes': array([], dtype=int64),\n",
       " 'detection_scores': array([], dtype=float32),\n",
       " 'detection_boxes': array([], shape=(0, 4), dtype=float32),\n",
       " 'num_detections': 0}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of how detections dictionary looks like.\n",
    "image_np = np.array(Image.open(TEST_IMAGE_PATHS[1]))\n",
    "detections = detect_objects_on_image(image_np, model)\n",
    "detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for image_path in TEST_IMAGE_PATHS:\n",
    "#     image_np = np.array(Image.open(image_path))\n",
    "#     detections = detect_objects_on_image(image_np, model)\n",
    "#     image_with_detections = draw_detections_on_image(image_np, detections, labels)\n",
    "#     plt.figure(figsize=(16, 12))\n",
    "#     plt.imshow(image_with_detections)\n",
    "\n",
    "\n",
    "# define a video capture object\n",
    "vid = cv2.VideoCapture(0)\n",
    "\"\"\"\n",
    "#binarizaci√≥n \n",
    "fgbg = cv2.createBackgroundSubtractorMOG2(history=20)\n",
    " \n",
    "# Deshabilitamos OpenCL, si no hacemos esto no funciona\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "\"\"\"\n",
    "\n",
    "while(1):\n",
    "      \n",
    "    # Capture the video frame\n",
    "    # by frame\n",
    "    ret, frame = vid.read()\n",
    "  \n",
    "    \n",
    "    # Display the resulting frame\n",
    "   \n",
    "    # Aplicamos el algoritmo\n",
    "    #fgmask = fgbg.apply(frame)\n",
    " \n",
    "\t# Copiamos el umbral para detectar los contornos\n",
    " \n",
    "   # cv2.imshow('Camara',frame)\n",
    "\t#cv2.imshow('Umbral',fgmask)\n",
    "\t\n",
    "\n",
    "\n",
    "    detections = detect_objects_on_image(frame, model)\n",
    "    image_with_detections = draw_detections_on_image(frame, detections, labels)\n",
    "      \n",
    "      \n",
    "    cv2.imshow('frame', image_with_detections)\n",
    "\n",
    "    # the 'q' button is set as the\n",
    "    # quitting button you may use any\n",
    "    # desired button of your choice\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "  \n",
    "# After the loop release the cap object\n",
    "vid.release()\n",
    "# Destroy all the windows\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the model to web-format"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the `ssdlite_mobilenet_v2_coco_2018_05_09` model on the web we need to convert it into the format that will be understandable by [tensorflowjs](https://www.tensorflow.org/js). To do so we may use [tfjs-converter](https://github.com/tensorflow/tfjs/tree/master/tfjs-converter) as following:\n",
    "\n",
    "```\n",
    "tensorflowjs_converter \\\n",
    "    --input_format=tf_saved_model \\\n",
    "    --output_format=tfjs_graph_model \\\n",
    "   ./experiments/objects_detection_ssdlite_mobilenet_v2/.tmp/datasets/ssdlite_mobilenet_v2_coco_2018_05_09/saved_model \\\n",
    "    ./demos/public/models/objects_detection_ssdlite_mobilenet_v2\n",
    "```\n",
    "\n",
    "Alternative and easier way would be to use a [@tensorflow-models/coco-ssd](https://www.npmjs.com/package/@tensorflow-models/coco-ssd) npm package. But just for exploration purpose let's go one level deeper and use the model directly without wrapper modules.\n",
    "\n",
    "You find this experiment in the [Demo app](https://trekhleb.github.io/machine-learning-experiments) and play around with it right in you browser to see how the model performs in real life."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
